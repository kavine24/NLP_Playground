{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "import ollama\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../../database/documents/2010.11929v2.pdf',\n",
    "         '../../database/documents/2212.06727v1.pdf',\n",
    "         '../../database/documents/2312.01232v2.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(['. ', ','],\n",
    "                                          keep_separator=True,\n",
    "                                          chunk_size=500,\n",
    "                                          chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OllamaEmbeddings(\n",
    "    model=\"llama3\"\n",
    ")\n",
    "\n",
    "embed_size = np.array(embedder.embed_query('huh')).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore already exists\n"
     ]
    }
   ],
   "source": [
    "if not client.collection_exists(\"computer_vision\"):\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=\"computer_vision\",\n",
    "        vectors_config=VectorParams(size=embed_size, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "    for file in files:\n",
    "        reader = PdfReader(file)\n",
    "\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "\n",
    "            documents = splitter.split_text(text=text)\n",
    "\n",
    "            embeddings = embedder.embed_documents(documents)\n",
    "\n",
    "            client.upsert(\n",
    "            collection_name=\"computer_vision\",\n",
    "                points=[\n",
    "                    PointStruct(\n",
    "                        id= idx + i,\n",
    "                        vector=embeddings[i],\n",
    "                        payload={\"document\": documents[i], 'file_name': file}\n",
    "                    )\n",
    "                    for i in range(len(documents))\n",
    "                ]\n",
    "            )\n",
    "            idx += len(documents)\n",
    "\n",
    "else:\n",
    "    print('vectorstore already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What are the recent advancements in computer vision'\n",
    "\n",
    "query_embed = np.array(embedder.embed_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", [43] is a large-scale image\n",
      "database created by researchers at Princeton University, ini-\n",
      "tially designed for the purpose of object recognition research\n",
      "in computer vision\n",
      ". The area\n",
      "of a circle represents the scale of each dataset (i.e., the number of images).\n",
      "previous GPT models, which were primarily designed\n",
      "for generating natural language text, iGPT has been\n",
      "trained to generate high-resolution images from textual\n",
      "descriptions.\n",
      "In Fig. 3, we present a chronological overview of recent\n",
      "representative work\n",
      "The Stand-Alone Self-Attention in Vision Models [18]\n",
      "marks an early departure from convolutional approaches in\n",
      "computer vision\n",
      ". The JFT-300M dataset is\n",
      "challenging due to its large size and the diversity of images\n",
      "and categories, making it a valuable resource for advancing\n",
      "research in the field of computer vision\n",
      ". Previous survey papers either put more effort\n",
      "into convolutional neural networks [33], [34], [35], [36] or\n",
      "focus on broader topics such as video processing [37], medical\n",
      "imaging [38], visual learning understanding [39] and object\n",
      "detection, action recognition, segmentation [40]. In this paper:\n",
      "•We review papers on attention mechanisms for image\n",
      "classification\n",
      ". We walk the readers through the recent\n",
      "advancements chronologically and systematically.\n",
      "•We comprehensively review papers on Vision Transform-\n",
      "ers for image classification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_response = client.query_points(\n",
    "    collection_name=\"computer_vision\", query=query_embed, limit=5\n",
    ")\n",
    "\n",
    "search_result = db_response.points\n",
    "\n",
    "docs = [result.payload['document'] for result in search_result]\n",
    "\n",
    "context = ''\n",
    "for doc in docs:\n",
    "    context += doc + '\\n'\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "\n",
      ", [43] is a large-scale image\n",
      "database created by researchers at Princeton University, ini-\n",
      "tially designed for the purpose of object recognition research\n",
      "in computer vision\n",
      ". The area\n",
      "of a circle represents the scale of each dataset (i.e., the number of images).\n",
      "previous GPT models, which were primarily designed\n",
      "for generating natural language text, iGPT has been\n",
      "trained to generate high-resolution images from textual\n",
      "descriptions.\n",
      "In Fig. 3, we present a chronological overview of recent\n",
      "representative work\n",
      "The Stand-Alone Self-Attention in Vision Models [18]\n",
      "marks an early departure from convolutional approaches in\n",
      "computer vision\n",
      ". The JFT-300M dataset is\n",
      "challenging due to its large size and the diversity of images\n",
      "and categories, making it a valuable resource for advancing\n",
      "research in the field of computer vision\n",
      ". Previous survey papers either put more effort\n",
      "into convolutional neural networks [33], [34], [35], [36] or\n",
      "focus on broader topics such as video processing [37], medical\n",
      "imaging [38], visual learning understanding [39] and object\n",
      "detection, action recognition, segmentation [40]. In this paper:\n",
      "•We review papers on attention mechanisms for image\n",
      "classification\n",
      ". We walk the readers through the recent\n",
      "advancements chronologically and systematically.\n",
      "•We comprehensively review papers on Vision Transform-\n",
      "ers for image classification\n",
      "\n",
      "\n",
      "Question: What are the recent advancements in computer vision\n",
      "\n",
      "Helpful Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \\\n",
    "\"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "formmatted_prompt = prompt_template.format(context=context, question=query)\n",
    "\n",
    "print(formmatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Stand-Alone Self-Attention in Vision Models [18] marks an early departure from convolutional approaches in computer vision. The JFT-300M dataset is challenging due to its large size and diversity of images and categories, making it a valuable resource for advancing research in the field of computer vision.\n"
     ]
    }
   ],
   "source": [
    "llm_response = ollama.generate(\n",
    "    model='llama3',\n",
    "    prompt=formmatted_prompt\n",
    ")\n",
    "\n",
    "print(llm_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_playgrounds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
